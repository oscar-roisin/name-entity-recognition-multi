{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER - Multilang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>ner</th>\n",
       "      <th>token_shape</th>\n",
       "      <th>ner_cat</th>\n",
       "      <th>pos_cat</th>\n",
       "      <th>token_shape_cat</th>\n",
       "      <th>pos_prev_cat</th>\n",
       "      <th>pos_prev_prev_cat</th>\n",
       "      <th>pos_next_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>PP</td>\n",
       "      <td>O</td>\n",
       "      <td>X</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sin</td>\n",
       "      <td>PS</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>första</td>\n",
       "      <td>RO</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reaktion</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>på</td>\n",
       "      <td>PP</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token pos ner token_shape  ner_cat  pos_cat  token_shape_cat  \\\n",
       "0         I  PP   O           X        6       18                0   \n",
       "1       sin  PS   O           x        6       19                5   \n",
       "2    första  RO   O           x        6       21                5   \n",
       "3  reaktion  NN   O           x        6       12                5   \n",
       "4        på  PP   O           x        6       18                5   \n",
       "\n",
       "   pos_prev_cat  pos_prev_prev_cat  pos_next_cat  \n",
       "0            10                 15            19  \n",
       "1            18                 10            21  \n",
       "2            19                 18            12  \n",
       "3            21                 19            18  \n",
       "4            12                 21            12  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR=\"../DATA\"\n",
    "TOKEN_COL=\"token\"\n",
    "TOKEN_SHAPE_COL=\"token_shape\"\n",
    "POS_COL=\"pos\"\n",
    "NER_COL=\"ner\"\n",
    "\n",
    "TOKEN_SHAPE_CAT_COL=\"token_shape_cat\"\n",
    "POS_CAT_COL=\"pos_cat\"\n",
    "POS_PREV_CAT_COL=\"pos_prev_cat\"\n",
    "POS_NEXT_CAT_COL=\"pos_next_cat\"\n",
    "POS_PREV_PREV_CAT_COL=\"pos_prev_prev_cat\"\n",
    "NER_CAT_COL=\"ner_cat\"\n",
    "\n",
    "swe_data = pd.read_csv( DATA_DIR + '/swedish.txt', sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "eng_data = pd.read_csv( DATA_DIR + '/english.txt', sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "fr_data = pd.read_csv( DATA_DIR + '/french.txt', sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "def analyseTokens(df):\n",
    "    #tokens = [\"\\\"\",\"'\",\",\",\".\",\"-\",\"_\"]\n",
    "    new_token_col = []\n",
    "    for token in df[TOKEN_COL]:\n",
    "        token=token.strip()\n",
    "        token_shape = \"\"\n",
    "        if len(token) == 1 and not token.isalnum():\n",
    "            token_shape = \"sign\" # or =token\n",
    "        else:\n",
    "            if token.isupper():\n",
    "                token_shape=\"X\"\n",
    "            elif token[0].isupper():\n",
    "                token_shape=\"Xx\"\n",
    "            else:\n",
    "                token_shape=\"x\"\n",
    "            if token[len(token)-1] == \".\":\n",
    "                token_shape+=\".\"\n",
    "        new_token_col.append(token_shape)\n",
    "    #print(len(new_token_col))\n",
    "    df[TOKEN_SHAPE_COL]=new_token_col\n",
    "\n",
    "def offsetArray(array,offset):\n",
    "    return np.roll(array,offset)\n",
    "    \n",
    "dataframes = [ swe_data ]#, eng_data, fr_data ]\n",
    "for df in dataframes:\n",
    "    df.columns = [ TOKEN_COL, POS_COL, NER_COL ]\n",
    "    analyseTokens(df)\n",
    "    df[NER_COL] = pd.Categorical(df[NER_COL])\n",
    "    df[NER_CAT_COL] = df[NER_COL].cat.codes\n",
    "    df[POS_COL] = pd.Categorical(df[POS_COL])\n",
    "    df[POS_CAT_COL] = df[POS_COL].cat.codes\n",
    "    df[TOKEN_SHAPE_COL] = pd.Categorical(df[TOKEN_SHAPE_COL])\n",
    "    df[TOKEN_SHAPE_CAT_COL] = df[TOKEN_SHAPE_COL].cat.codes\n",
    "    \n",
    "    df[POS_PREV_CAT_COL] = offsetArray(df[POS_CAT_COL],1)\n",
    "    df[POS_PREV_PREV_CAT_COL] = offsetArray(df[POS_CAT_COL],2)\n",
    "    df[POS_NEXT_CAT_COL] = offsetArray(df[POS_CAT_COL],-1)\n",
    "    \n",
    "#     POS_NEXT_CAT_COL\n",
    "#     POS_PREV_PREV_CAT_COL\n",
    "    \n",
    "\n",
    "# for data in swe_data[POS_CAT_COL].head():\n",
    "#     print(data)\n",
    "\n",
    "swe_POS_max = max(swe_data[POS_CAT_COL].head())\n",
    "swe_NER_max = max(swe_data[NER_CAT_COL].head())\n",
    "swe_SHAPE_max = max(swe_data[TOKEN_SHAPE_CAT_COL].head())\n",
    "    \n",
    "swe_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  0 10 15 19]\n",
      " [19  5 18 10 21]\n",
      " [21  5 19 18 12]\n",
      " ...\n",
      " [24  5  4 11 12]\n",
      " [12  5 24  4 18]\n",
      " [18  5 12 24 16]]\n",
      "[6 6 6 ... 6 6 6]\n",
      "[16  2 24 12 12]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Input dim:5\n",
      "Output dim:23\n"
     ]
    }
   ],
   "source": [
    "drop_cols =  [TOKEN_COL, POS_COL, NER_COL, TOKEN_SHAPE_COL]\n",
    "#drop_cols =  [TOKEN_COL]\n",
    "\n",
    "#swe_data.dtypes\n",
    "#swe_data[POS_COL] = pd.Categorical(swe_data[POS_COL])\n",
    "#swe_data[POS_COL] = swe_data.pos.cat.codes\n",
    "#swe_data.head()\n",
    "df=swe_data.copy()\n",
    "\n",
    "for drop_col in drop_cols:\n",
    "    df.pop(drop_col)\n",
    "df = df.head(10000)\n",
    "target = df.pop(NER_CAT_COL)\n",
    "\n",
    "\n",
    "\n",
    "#dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
    "x=df.values\n",
    "y=target.values\n",
    "print(df.values)\n",
    "print(target.values)\n",
    "DATASET_LENGTH=len(target.values)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical(y)\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y_binary, test_size = 0.3, random_state = 0, shuffle=False) #Remove shuffle\n",
    "\n",
    "print(xTrain[8])\n",
    "print(yTrain[8])\n",
    "\n",
    "#for feat, targ in dataset.take(5):\n",
    "#  print ('Features: {}, Target: {}'.format(feat, targ))\n",
    "print(yTrain)\n",
    "\n",
    "INPUT_DIM=len(xTrain[0])\n",
    "print(\"Input dim:\" + str(INPUT_DIM))\n",
    "OUTPUT_DIM=max([np.argmax(i) for i in yTrain])+1\n",
    "print(\"Output dim:\" + str(OUTPUT_DIM))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#categorical_vars = [POS_COL, NER_COL, TOKEN_SHAPE_COL]\n",
    "#numerical_vars =[]\n",
    "#numerical_vars = ['age', 'siblings_spouses_aboard', 'parents_children_aboard', 'fare']\n",
    "#text_vars = ['name']\n",
    "#text_vars = []\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train, test = train_test_split(df, test_size=0.3)\n",
    "#print(train.head())\n",
    "#trainLabels = train.as_matrix(columns=[POS_COL])\n",
    "#train = train.as_matrix(columns=['vectorized'])\n",
    "#print(train)\n",
    "#testLabels = test.as_matrix(columns=['label'])\n",
    "#test = test.as_matrix(columns=['vectorized'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/32\n",
      "7000/7000 [==============================] - 1s 196us/sample - loss: 0.6443 - recall_32: 0.7073\n",
      "Epoch 2/32\n",
      "7000/7000 [==============================] - 1s 135us/sample - loss: 0.2334 - recall_32: 0.8844\n",
      "Epoch 3/32\n",
      "7000/7000 [==============================] - 1s 128us/sample - loss: 0.2168 - recall_32: 0.9007\n",
      "Epoch 4/32\n",
      "7000/7000 [==============================] - 1s 171us/sample - loss: 0.2123 - recall_32: 0.9063\n",
      "Epoch 5/32\n",
      "7000/7000 [==============================] - 1s 135us/sample - loss: 0.2119 - recall_32: 0.9101\n",
      "Epoch 6/32\n",
      "7000/7000 [==============================] - 1s 192us/sample - loss: 0.2082 - recall_32: 0.9122\n",
      "Epoch 7/32\n",
      "7000/7000 [==============================] - 1s 108us/sample - loss: 0.2045 - recall_32: 0.9144\n",
      "Epoch 8/32\n",
      "7000/7000 [==============================] - 1s 125us/sample - loss: 0.2006 - recall_32: 0.9159\n",
      "Epoch 9/32\n",
      "7000/7000 [==============================] - 1s 114us/sample - loss: 0.1996 - recall_32: 0.9167\n",
      "Epoch 10/32\n",
      "7000/7000 [==============================] - 1s 121us/sample - loss: 0.1959 - recall_32: 0.9179\n",
      "Epoch 11/32\n",
      "7000/7000 [==============================] - 1s 129us/sample - loss: 0.1944 - recall_32: 0.9187\n",
      "Epoch 12/32\n",
      "7000/7000 [==============================] - 1s 119us/sample - loss: 0.1938 - recall_32: 0.9194\n",
      "Epoch 13/32\n",
      "7000/7000 [==============================] - 1s 135us/sample - loss: 0.1941 - recall_32: 0.9202\n",
      "Epoch 14/32\n",
      "7000/7000 [==============================] - 1s 140us/sample - loss: 0.1909 - recall_32: 0.9205\n",
      "Epoch 15/32\n",
      "7000/7000 [==============================] - 1s 123us/sample - loss: 0.1874 - recall_32: 0.9213\n",
      "Epoch 16/32\n",
      "7000/7000 [==============================] - 1s 177us/sample - loss: 0.1896 - recall_32: 0.9216\n",
      "Epoch 17/32\n",
      "7000/7000 [==============================] - 1s 183us/sample - loss: 0.1880 - recall_32: 0.9220\n",
      "Epoch 18/32\n",
      "7000/7000 [==============================] - 1s 173us/sample - loss: 0.1867 - recall_32: 0.9224\n",
      "Epoch 19/32\n",
      "7000/7000 [==============================] - 1s 189us/sample - loss: 0.1837 - recall_32: 0.9227\n",
      "Epoch 20/32\n",
      "7000/7000 [==============================] - 1s 166us/sample - loss: 0.1829 - recall_32: 0.9233\n",
      "Epoch 21/32\n",
      "7000/7000 [==============================] - 1s 186us/sample - loss: 0.1813 - recall_32: 0.9237\n",
      "Epoch 22/32\n",
      "7000/7000 [==============================] - 1s 180us/sample - loss: 0.1808 - recall_32: 0.9241\n",
      "Epoch 23/32\n",
      "7000/7000 [==============================] - 1s 184us/sample - loss: 0.1769 - recall_32: 0.9244\n",
      "Epoch 24/32\n",
      "7000/7000 [==============================] - 2s 291us/sample - loss: 0.1785 - recall_32: 0.9249\n",
      "Epoch 25/32\n",
      "7000/7000 [==============================] - 1s 191us/sample - loss: 0.1770 - recall_32: 0.9251\n",
      "Epoch 26/32\n",
      "7000/7000 [==============================] - 1s 196us/sample - loss: 0.1747 - recall_32: 0.9254\n",
      "Epoch 27/32\n",
      "7000/7000 [==============================] - 1s 189us/sample - loss: 0.1730 - recall_32: 0.9257\n",
      "Epoch 28/32\n",
      "7000/7000 [==============================] - 1s 204us/sample - loss: 0.1729 - recall_32: 0.9260\n",
      "Epoch 29/32\n",
      "7000/7000 [==============================] - 1s 205us/sample - loss: 0.1723 - recall_32: 0.9263\n",
      "Epoch 30/32\n",
      "7000/7000 [==============================] - 1s 179us/sample - loss: 0.1716 - recall_32: 0.9266\n",
      "Epoch 31/32\n",
      "7000/7000 [==============================] - 1s 177us/sample - loss: 0.1689 - recall_32: 0.9269\n",
      "Epoch 32/32\n",
      "7000/7000 [==============================] - 1s 187us/sample - loss: 0.1684 - recall_32: 0.9272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7d7c9fc320>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_compiled_model():\n",
    "#   model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(10, activation='relu', input_shape=(2,)),\n",
    "#     tf.keras.layers.Dense(10, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "#   ])\n",
    "\n",
    "#   model.compile(optimizer='adam',\n",
    "#                 loss='categorical_crossentropy',\n",
    "#                 metrics=['accuracy'])\n",
    "#   return model\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(142, input_dim=INPUT_DIM, activation='relu'))\n",
    "#model.add(tf.keras.layers.Dense(22, activation='relu'))\n",
    "#model.add(tf.keras.layers.Dense(22, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(OUTPUT_DIM, activation='softmax'))\n",
    "# Compile model\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.Recall()])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.Precision()])\n",
    "\n",
    "# https://appliedmachinelearning.blog/2019/04/01/training-deep-learning-based-named-entity-recognition-from-scratch-disease-extraction-hackathon/\n",
    "\n",
    "\n",
    "#model = get_compiled_model()\n",
    "model.fit(xTrain,yTrain, epochs=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5 24 24 12]\n",
      " [12  5  1 24  2]\n",
      " [ 2  5 12  1 24]]\n",
      "6\n",
      "1.0\n",
      "6\n",
      "0.9999255\n",
      "6\n",
      "1.0\n",
      "Features|Prediction|Target\n",
      "[16  2 22  2  0] 1 1\n",
      "[16  2 24  9 22] 11 20\n",
      "[16  2 18 12 24] 11 1\n",
      "[16  2 24 24 18] 11 17\n",
      "[14  2 10 24 12] 11 6\n",
      "[16  2 12  8 12] 11 9\n",
      "[16  2 12 16 24] 11 9\n",
      "[23  2 13 16 23] 11 6\n",
      "[23  2 23 13 23] 11 6\n",
      "[23  2 23 23 13] 1 6\n",
      "[16  2 12 24 10] 11 11\n",
      "[16  2 12  8 24] 11 11\n",
      "[16  2 18 12 18] 11 1\n",
      "[16  0 18 16 10] 1 1\n",
      "[16  2 18 12 11] 11 1\n",
      "[16  2 18 12 11] 11 1\n",
      "[16  2 11 16  9] 1 1\n",
      "[16  2  9 16 18] 11 1\n",
      "[16  2 12  8 16] 11 11\n",
      "[16  2 16 12 11] 11 11\n",
      "[16  2 24 12 16] 11 11\n",
      "[16  2 16 24  4] 1 11\n",
      "[12  2 18 12 13] 11 6\n",
      "[16  0 10 13 16] 11 11\n",
      "[16  0 16 10  1] 1 11\n",
      "[12  2 18 17 11] 11 0\n",
      "[16  2 12 24 16] 11 11\n",
      "[16  2 16 12 18] 11 11\n",
      "[16  2 18 16 10] 11 9\n",
      "[18  2 10 16 16] 11 6\n",
      "[16  2 18 10 12] 11 1\n",
      "[16  2 24  6  9] 11 11\n",
      "[16  2 16 12 10] 11 17\n",
      "[16  2 18  0  9] 1 1\n",
      "[16  2 10 12 16] 11 1\n",
      "[16  2 16 10 13] 11 1\n",
      "[16  2 13 16 13] 11 17\n",
      "[16  0 18 14 10] 1 17\n",
      "[16  2 18 24  9] 11 11\n",
      "[18  2 10 14 20] 11 6\n",
      "[16  2 12  8 12] 11 9\n",
      "[16  2 12 16 12] 11 9\n",
      "[16  2 18 12  9] 1 1\n",
      "[16  0 10 17 16] 11 11\n",
      "[16  0 16 10  8] 1 11\n",
      "[16  0 12 12 16] 11 11\n",
      "[16  0 16 12 17] 11 11\n",
      "[17  2 16 16 24] 11 6\n",
      "[16  2 18 12 18] 11 1\n",
      "[16  2 18 17  0] 1 1\n",
      "[16  2 22 10 24] 1 20\n",
      "[16  2 18 12 18] 11 1\n",
      "[16  2 12 24 16] 11 11\n",
      "[16  2 16 12 18] 11 11\n",
      "[16  2 18 16 10] 11 9\n",
      "[14  2 10 12 12] 11 6\n",
      "[12  2 18 17 11] 11 1\n",
      "[16  2 12 24 16] 11 11\n",
      "[16  2 16 12  4] 1 11\n",
      "[16  2 18 15 20] 11 1\n",
      "[16  2 18  0 18] 11 1\n",
      "[16  2 18 16 10] 11 1\n",
      "[16  0 18 12 11] 11 17\n",
      "[16  2 12 24 16] 11 11\n",
      "[16  2 16 12 18] 11 11\n",
      "[16  2 18 16 10] 11 9\n",
      "[16  2 18 12 24] 11 1\n",
      "[16  2 18 12 10] 1 1\n",
      "[16  0 14 11 12] 11 19\n",
      "[16  2 18 12 10] 1 1\n",
      "[18  2 24 12 20] 11 6\n",
      "[16  2 24 12 12] 11 17\n",
      "[16  2 18 12  9] 1 1\n",
      "[16  0 18 12  4] 1 21\n",
      "[16  2 12 24 10] 11 11\n",
      "[16  2 18 14 18] 11 1\n",
      "[16  2 12 24 18] 11 11\n",
      "[16  2 12  8 16] 11 11\n",
      "[16  2 16 12 13] 11 11\n",
      "[16  0 13 16 13] 11 17\n",
      "[16  2 12 18 11] 11 20\n",
      "[12  2 18 12  0] 1 6\n",
      "[16  0 18  0 10] 1 1\n",
      "[16  2 10 16 16] 11 11\n",
      "[16  2 16 10 11] 11 11\n",
      "[16  0 18 12 24] 11 21\n",
      "[12  2 18  9 20] 1 1\n",
      "[12  2 20 12 10] 11 6\n",
      "[16  2 10 12 16] 11 11\n",
      "[16  2 16 10 24] 11 11\n",
      "[16  2 12  8 16] 11 11\n",
      "[16  2 16 12 16] 11 11\n",
      "[16  2 16 16 11] 11 11\n",
      "[16  2 12 12 16] 11 11\n",
      "[16  2 16 12 16] 11 11\n",
      "[16  2 16 16 18] 11 11\n",
      "[16  2 12  8 16] 11 11\n",
      "[16  2 16 12 18] 11 11\n",
      "[16  2 18 16 16] 11 1\n",
      "[16  2 16 18 10] 11 1\n",
      "[16  0 22 17  9] 1 17\n",
      "[12  2  1 24 10] 11 6\n",
      "[16  2 18 12 16] 11 11\n",
      "[16  2 16 18 18] 11 11\n",
      "[16  2 18 12 24] 11 1\n",
      "[16  2 12  8 11] 11 1\n",
      "[16  2 11 16 17] 11 1\n",
      "[17  2 16 11 24] 11 6\n",
      "[16  2 18 12  0] 1 1\n",
      "[16  2 18 12 10] 1 1\n",
      "[12  2 10 12 14] 11 6\n",
      "[16  2 18 24 24] 11 1\n",
      "[16  0 18 12 18] 11 17\n",
      "[16  2 18 16 10] 11 1\n",
      "[16  2 12 24 10] 11 11\n",
      "[16  2 12  8 16] 11 11\n",
      "[16  2 16 12 11] 11 11\n",
      "[16  2 12  0 12] 11 11\n",
      "[16  2 24 11 12] 11 19\n",
      "[12  2 11 12 11] 11 6\n",
      "[12  0 11 12 20] 11 6\n",
      "[18  2 11  0 17] 11 6\n",
      "[16  2 18 10 24] 11 1\n",
      "[12  2 10 12 12] 11 6\n",
      "[16  2 22 24 18] 11 1\n",
      "[16  2 22 10 24] 1 1\n",
      "[16  2 14  1 19] 11 1\n",
      "[16  0 10  0 16] 11 11\n",
      "[16  0 16 10  1] 1 11\n",
      "[16  2 24  0 16] 11 11\n",
      "[16  2 16 24  9] 11 11\n",
      "[12  2 19  9 18] 1 9\n",
      "[17  2 10 12 17] 11 6\n",
      "[16  2 13 12 14] 11 1\n",
      "[16  2 18 12 16] 11 1\n",
      "[16  2 16 18 16] 11 1\n",
      "[16  2 16 16 11] 11 1\n",
      "[16  2 19 18 16] 11 1\n",
      "[16  2 16 19 24] 11 1\n",
      "[18  2 10 18 16] 11 6\n",
      "[16  0 18 10 16] 11 11\n",
      "[16  0 16 18 16] 11 11\n",
      "[16  2 16 16 12] 11 11\n",
      "[17  2 12 16 24] 11 6\n",
      "[12  2 13 10 13] 11 6\n",
      "[16  2 13 12  9] 1 1\n",
      "[16  2  9 16 16] 11 1\n",
      "[16  2 16  9 24] 11 1\n",
      "[16  2 12 24  0] 11 1\n",
      "[16  2 18 12 11] 11 1\n",
      "[16  2 22 24  0] 1 1\n",
      "[16  2 18 12  9] 1 11\n",
      "[16  2  9 16 16] 11 11\n",
      "[16  2 16  9  9] 1 11\n",
      "[16  2 12  8 16] 11 11\n",
      "[16  2 16 12 16] 11 11\n",
      "[16  2 16 16 10] 11 11\n",
      "[16  2 18 11  9] 1 1\n",
      "[16  2 18 12 24] 11 1\n",
      "[18  2 17 24  1] 1 6\n",
      "[16  2 18 12 12] 11 11\n",
      "[16  2 18 12 24] 11 1\n",
      "[16  2 18 10  8] 1 17\n",
      "[16  2 24 14 12] 11 11\n",
      "[16  2 18 12 16] 11 11\n",
      "[16  2 16 18 24] 11 11\n",
      "[16  2 18  0 16] 11 1\n",
      "[16  2 16 18 10] 11 1\n",
      "[12  2 10 16 12] 11 6\n",
      "[16  2 18 24  9] 11 1\n",
      "[16  2 18 17  4] 1 1\n",
      "[16  2 24 17 10] 1 1\n",
      "[16  2 18 12 10] 1 1\n",
      "[16  2 18 24 11] 11 1\n",
      "[16  2 24 11 10] 11 11\n",
      "[16  0 24  0  0] 1 17\n",
      "[16  2 24 17 11] 11 1\n",
      "[16  0 10 12 10] 1 1\n",
      "[18  0 10 16  8] 1 6\n",
      "[16  2 18 12 10] 1 1\n",
      "[16  2 12 10 16] 11 9\n",
      "[16  2 16 12 11] 11 9\n"
     ]
    }
   ],
   "source": [
    "threeDataPoints = xTest[:3]\n",
    "print(threeDataPoints)\n",
    "predictions = model.predict(threeDataPoints)\n",
    "for prediction in predictions:\n",
    "    index = np.argmax(prediction)\n",
    "    print(index)\n",
    "    print(prediction[index])\n",
    "\n",
    "predictions = model.predict(xTest)\n",
    "print(\"Features|Prediction|Target\")\n",
    "for i in range(len(predictions)):\n",
    "    index = np.argmax(predictions[i])\n",
    "    if index != 6:\n",
    "        #only 6's\n",
    "        print(str(xTest[i]) + \" \" + str(index) + \" \" + str(np.argmax(yTest[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss, test acc, test recall: [0.22716047498087089, 0.9273468]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(xTest,yTest, verbose=0)\n",
    "print('test loss, test acc, test recall:', results)\n",
    "\n",
    "#Only Acc, 142, 7000, feat:1,2,-1 : [0.22299354140212138, 0.9714275]\n",
    "#[0.2284703826159239, 0.9709106, 0.92750543]\n",
    "#Recall [0.22716047498087089, 0.9273468]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/floydhub/named-entity-recognition-template/blob/master/ner.ipynb\n",
    "# Saving Vocab\n",
    "#with open('models/word_to_index.pickle', 'wb') as handle:\n",
    "#    pickle.dump(word2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    " \n",
    "# Saving Vocab\n",
    "#with open('models/tag_to_index.pickle', 'wb') as handle:\n",
    "#    pickle.dump(tag2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Saving Model Weight\n",
    "#model.save_weights('models/lstm_crf_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
