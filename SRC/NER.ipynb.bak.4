{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER - Multilang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import keras\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Mainly Used:\n",
    "# https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# SWE-TAGSET:[\"ORG\",\"PRS\",\"LOC\",\"OTHER\",\"O\"] \n",
    "def sweStandardizeTag(line):\n",
    "    SWE_TAG_SET=[\"ORG\",\"PRS\",\"LOC\",\"OTHER\",\"O\"]\n",
    "    lineParts = line.split(\"\\t\")\n",
    "    ner_tag = lineParts[2]\n",
    "    if ner_tag in SWE_TAG_SET:\n",
    "        return line\n",
    "    if ner_tag[:3] == \"LOC\" or ner_tag == \"place\":\n",
    "        ner_tag = \"LOC\"\n",
    "    if ner_tag == \"person\":\n",
    "        ner_tag = \"PRS\"\n",
    "    if ner_tag in [\"inst\", \"ORG/PRS\", \"OBJ/ORG\"]:\n",
    "        ner_tag = \"ORG\"\n",
    "    if ner_tag in [\"WRK\",\"OBJ\",\"EVN\",\"product\",\"other\",\"work\",\"event\",\"myth\",\"animal\",\"MSR\",\"TME\",\"PRS/WRK\"]:\n",
    "        ner_tag = \"OTHER\"\n",
    "    \n",
    "    if ner_tag not in SWE_TAG_SET:\n",
    "        raise Exception(ner_tag + \" - not in tag set\") \n",
    "    return lineParts[0] + \"\\t\" + lineParts[1] + \"\\t\" + ner_tag\n",
    "\n",
    "\n",
    "DATA_DIR=\"../DATA\"\n",
    "filenames = [ \"swedish.txt\"]#, \"french.txt\", \"english.txt\"]\n",
    "for filename in filenames:\n",
    "    originalFile= open(DATA_DIR + \"/\" + filename,\"r\")\n",
    "    file= open(DATA_DIR + \"/sent_\" + filename,\"w+\")\n",
    "    originalFileLines =originalFile.readlines()\n",
    "    sentId=1\n",
    "    for line in originalFileLines:\n",
    "        line = line.strip() # problem with this?\n",
    "        if len(line) == 0:\n",
    "            sentId = sentId + 1\n",
    "            continue\n",
    "        line = sweStandardizeTag(line)\n",
    "        file.write(str(sentId) + \"\\t\" + line +\"\\n\")\n",
    "# for i in range(10):\n",
    "#      file.write(\"This is line %d\\r\\n\" % (i+1))\n",
    "    file.close()\n",
    "    originalFile.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>ner</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>PP</td>\n",
       "      <td>O</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sin</td>\n",
       "      <td>PS</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>första</td>\n",
       "      <td>RO</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>reaktion</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>på</td>\n",
       "      <td>PP</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent     token pos ner token_shape\n",
       "0     1         I  PP   O           X\n",
       "1     1       sin  PS   O           x\n",
       "2     1    första  RO   O           x\n",
       "3     1  reaktion  NN   O           x\n",
       "4     1        på  PP   O           x"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR=\"../DATA\"\n",
    "SENT_COL=\"sent\"\n",
    "TOKEN_COL=\"token\"\n",
    "SHAPE_COL=\"token_shape\"\n",
    "POS_COL=\"pos\"\n",
    "NER_COL=\"ner\"\n",
    "\n",
    "TOKEN_SHAPE_CAT_COL=\"token_shape_cat\"\n",
    "POS_CAT_COL=\"pos_cat\"\n",
    "POS_PREV_CAT_COL=\"pos_prev_cat\"\n",
    "POS_NEXT_CAT_COL=\"pos_next_cat\"\n",
    "POS_PREV_PREV_CAT_COL=\"pos_prev_prev_cat\"\n",
    "NER_CAT_COL=\"ner_cat\"\n",
    "\n",
    "filenames = [ \"sent_swedish.txt\", \"sent_english.txt\", \"sent_french.txt\"]\n",
    "\n",
    "swe_data = pd.read_csv( DATA_DIR + '/' + filenames[0], sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "eng_data = pd.read_csv( DATA_DIR + '/' + filenames[1], sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "fr_data = pd.read_csv( DATA_DIR + '/' + filenames[2], sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "def analyseTokens(df):\n",
    "    #tokens = [\"\\\"\",\"'\",\",\",\".\",\"-\",\"_\"]\n",
    "    new_token_col = []\n",
    "    for token in df[TOKEN_COL]:\n",
    "        #print(token)\n",
    "        token=token.strip()\n",
    "        token_shape = \"\"\n",
    "        if len(token) == 1 and not token.isalnum():\n",
    "            token_shape = \"sign\" # or =token\n",
    "        else:\n",
    "            if token.isupper():\n",
    "                token_shape=\"X\"\n",
    "            elif token[0].isupper():\n",
    "                token_shape=\"Xx\"\n",
    "            else:\n",
    "                token_shape=\"x\"\n",
    "            if token[len(token)-1] == \".\":\n",
    "                token_shape+=\".\"\n",
    "        new_token_col.append(token_shape)\n",
    "    #print(len(new_token_col))\n",
    "    df[SHAPE_COL]=new_token_col\n",
    "\n",
    "def offsetArray(array,offset):\n",
    "    return np.roll(array,offset)\n",
    "    \n",
    "dataframes = [ swe_data, eng_data, fr_data ]\n",
    "dataframes = [ swe_data ] #TODO remove\n",
    "\n",
    "for df in dataframes:\n",
    "    df.columns = [ SENT_COL, TOKEN_COL, POS_COL, NER_COL ]\n",
    "    analyseTokens(df)\n",
    "\n",
    "# words = list(set(swe_data[\"token\"].values))\n",
    "# n_words = len(words);\n",
    "# print(n_words)\n",
    "\n",
    "DATA_TESTED = swe_data.head(500000)\n",
    "words = DATA_TESTED[TOKEN_COL].values.tolist()\n",
    "tags = DATA_TESTED[NER_COL].values.tolist()\n",
    "\n",
    "swe_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 1\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            sub_df = self.df[self.df[SENT_COL] == self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return sub_df[TOKEN_COL].values.tolist(), sub_df[POS_COL].values.tolist(), sub_df[NER_COL].values.tolist()    \n",
    "        except:\n",
    "            self.empty = True\n",
    "            print(\"EMPTY\")\n",
    "            return None, None, None\n",
    "getter = SentenceGetter(swe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEMPLATE FOR TESTING ENG/FR\n",
    "\n",
    "# SWE-TAGSET:[\"ORG\",\"PRS\",\"LOC\",\"OTHER\",\"O\"] \n",
    "# TODO for END / FR\n",
    "# sub_df = swe_data[swe_data[NER_COL] == \"OTHER\"]\n",
    "# print(sub_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_map(word):\n",
    "    '''Simple feature map.'''\n",
    "    return np.array([word.istitle(), word.islower(), word.isupper(), len(word),\n",
    "                     word.isdigit(),  word.isalpha()])\n",
    "\n",
    "## UNCOMMENT FOR SIMPLE CLASSIFICATION\n",
    "# words = [feature_map(w) for w in DATA_TESTED[TOKEN_COL].values.tolist()]\n",
    "# pred = cross_val_predict(RandomForestClassifier(n_estimators=20),\n",
    "#                          X=words, y=tags, cv=5)\n",
    "# report = classification_report(y_pred=pred, y_true=tags)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t,s) for w, p, t,s in zip(s[TOKEN_COL].values.tolist(),\n",
    "                                                           s[POS_COL].values.tolist(),\n",
    "                                                           s[NER_COL].values.tolist(),\n",
    "                                                           s[SHAPE_COL].values.tolist())]\n",
    "        self.grouped = self.data.groupby(SENT_COL).apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "# def word2features(sent, i):\n",
    "#     word = sent[i][0]\n",
    "#     postag = sent[i][1]\n",
    "\n",
    "#     features = {\n",
    "#         'bias': 1.0,\n",
    "#         'word.lower()': word.lower(),\n",
    "#         'word[-3:]': word[-3:],\n",
    "#         'word[-2:]': word[-2:],\n",
    "#         'word.isupper()': word.isupper(),\n",
    "#         'word.istitle()': word.istitle(),\n",
    "#         'word.isdigit()': word.isdigit(),\n",
    "#         'postag': postag,\n",
    "#         'postag[:2]': postag[:2],\n",
    "#     }\n",
    "#     if i > 0:\n",
    "#         word1 = sent[i-1][0]\n",
    "#         postag1 = sent[i-1][1]\n",
    "#         features.update({\n",
    "#             '-1:word.lower()': word1.lower(),\n",
    "#             '-1:word.istitle()': word1.istitle(),\n",
    "#             '-1:word.isupper()': word1.isupper(),\n",
    "#             '-1:postag': postag1,\n",
    "#             '-1:postag[:2]': postag1[:2],\n",
    "#         })\n",
    "#     else:\n",
    "#         features['BOS'] = True\n",
    "\n",
    "#     if i < len(sent)-1:\n",
    "#         word1 = sent[i+1][0]\n",
    "#         postag1 = sent[i+1][1]\n",
    "#         features.update({\n",
    "#             '+1:word.lower()': word1.lower(),\n",
    "#             '+1:word.istitle()': word1.istitle(),\n",
    "#             '+1:word.isupper()': word1.isupper(),\n",
    "#             '+1:postag': postag1,\n",
    "#             '+1:postag[:2]': postag1[:2],\n",
    "#         })\n",
    "#     else:\n",
    "#         features['EOS'] = True\n",
    "\n",
    "#     return features\n",
    "\n",
    "\n",
    "# def sent2features(sent):\n",
    "#     return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "# def sent2labels(sent):\n",
    "#     return [label for token, postag, label in sent]\n",
    "\n",
    "# def sent2tokens(sent):\n",
    "#     return [token for token, postag, label in sent]\n",
    "\n",
    "# X = [sent2features(s) for s in sentences]\n",
    "# y = [sent2labels(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data into Train/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "https://www.depends-on-the-definition.com/guide-sequence-tagging-neural-networks-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of sentencces: 31855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len:  121\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(DATA_TESTED)\n",
    "sent = getter.get_next()\n",
    "# print(sent)\n",
    "sentences = getter.sentences\n",
    "print(\"no. of sentencces: \" +str(len(sentences)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.show()\n",
    "\n",
    "print(\"max len: \",max([len(s) for s in sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "{'LOC': 0, 'PRS': 1, 'ORG': 2, 'O': 3, 'OTHER': 4}\n",
      "{'PN': 0, 'DT': 1, 'MAD': 2, 'IE': 3, 'IN': 4, 'PC': 5, 'HP': 6, 'JJ': 7, 'RO': 8, 'HD': 9, 'VB': 10, 'NN': 11, 'MID': 12, 'HS': 13, 'AB': 14, 'RG': 15, 'PS': 16, 'HA': 17, 'PAD': 18, 'PP': 19, 'PL': 20, 'UO': 21, 'SN': 22, 'PM': 23, 'KN': 24, 'POSPAD': 25}\n",
      "{'X.': 0, 'x.': 1, 'sign': 2, 'X': 3, 'Xx.': 4, 'Xx': 5, 'x': 6, 'SHAPEPAD': 7}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "words = list(set(DATA_TESTED[TOKEN_COL].values))\n",
    "words.append(\"ENDPAD\")\n",
    "tags = list(set(DATA_TESTED[NER_COL].values))\n",
    "pos = list(set(DATA_TESTED[POS_COL].values))\n",
    "pos.append(\"POSPAD\")\n",
    "shape = list(set(DATA_TESTED[SHAPE_COL].values))\n",
    "shape.append(\"SHAPEPAD\")\n",
    "n_words = len(words)\n",
    "n_tags = len(tags)\n",
    "n_pos = len(pos)\n",
    "n_shape = len(shape)\n",
    "print(n_pos)\n",
    "# MAX_LEN = max([len(s) for s in sentences])\n",
    "MAX_LEN = 80\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "pos2idx = {t: i for i, t in enumerate(pos)}\n",
    "shape2idx = {t: i for i, t in enumerate(shape)}\n",
    "# print(word2idx[\"i\"])\n",
    "print(tag2idx)\n",
    "print(pos2idx)\n",
    "print(shape2idx)\n",
    "\n",
    "\n",
    "\n",
    "# n_words = len(words); n_words\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_word = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X_shape = [[shape2idx[w[3]] for w in s] for s in sentences]\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "X_pos = [[pos2idx[w[1]] for w in s] for s in sentences]\n",
    "\n",
    "def limitSentenceLength( arrays, max_len):\n",
    "    newArrays = []\n",
    "    for sent_array in arrays:\n",
    "        if(len(sent_array)>max_len):\n",
    "            newArrays.append(sent_array[:max_len])\n",
    "        else:\n",
    "            newArrays.append(sent_array)\n",
    "    return newArrays\n",
    "\n",
    "#Limit sentence to MAX_LEN\n",
    "y = limitSentenceLength(y, MAX_LEN)\n",
    "X_word = limitSentenceLength(X_word, MAX_LEN)\n",
    "X_pos = limitSentenceLength(X_pos, MAX_LEN)\n",
    "X_shape = limitSentenceLength(X_shape, MAX_LEN)\n",
    "\n",
    "#Pad to MAX_LEN\n",
    "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "X_word = pad_sequences(maxlen=MAX_LEN, sequences=X_word, padding=\"post\", value=n_words - 1)\n",
    "X_pos = pad_sequences(maxlen=MAX_LEN, sequences=X_pos, padding=\"post\", value=n_pos - 1)\n",
    "X_shape = pad_sequences(maxlen=MAX_LEN, sequences=X_pos, padding=\"post\", value=n_pos - 1)\n",
    "\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "\n",
    "X = np.column_stack((X_word,X_pos,X_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47047 20350 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402\n",
      " 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402\n",
      " 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402\n",
      " 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402\n",
      " 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402\n",
      " 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402 62402\n",
      " 62402 62402 62402 62402 62402 62402 62402 62402     7    11    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25\n",
      "    25    25    25    25     7    11    25    25    25    25    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25\n",
      "    25    25    25    25    25    25    25    25    25    25    25    25]\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 80, 20)       1248080     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 80, 20)       1248080     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 80, 100)      28400       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 80, 100)      28400       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 80, 5)        505         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 80, 5)        505         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 80, 5)        0           time_distributed_1[0][0]         \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 2,553,970\n",
      "Trainable params: 2,553,970\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25802 samples, validate on 2867 samples\n",
      "Epoch 1/3\n",
      "  224/25802 [..............................] - ETA: 17:23 - loss: 1.1246 - accuracy: 0.8627"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "\n",
    "# input1 = Input(shape=(MAX_LEN*3,))\n",
    "# # word_emb = Embedding(input_dim=n_words + 1, output_dim=80, input_length=MAX_LEN, mask_zero=True)(input)\n",
    "\n",
    "# # input2 = Input(shape=(MAX_LEN,))\n",
    "# # pos_emb = Embedding(input_dim=n_pos, output_dim=80, input_length=MAX_LEN, mask_zero=True)(input2)\n",
    "# # concate_layers = keras.layers.concatenate([word_emb, pos_emb],axis=1)\n",
    "\n",
    "# model = Sequential()\n",
    "# # model.add(Bidirectional(LSTM(num_channels, \n",
    "# #                                      implementation = 2, recurrent_activation = 'sigmoid'),\n",
    "# #                                 input_shape=(input_length, input_dim)))\n",
    "\n",
    "# model.add(Dense(MAX_LEN*3, activation='relu', input_dim=MAX_LEN*3))\n",
    "# model.add(Dense(n_tags, activation='sigmoid'))\n",
    "\n",
    "# main_input = Input(shape=(MAX_LEN,))\n",
    "# word_emb = Embedding(input_dim=n_words + 1, output_dim=150,input_length=MAX_LEN, mask_zero=True)(main_input)\n",
    "\n",
    "# input2 = Input(shape=(MAX_LEN,))\n",
    "# posd_emb = Embedding(input_dim=n_pos, output_dim=150,input_length=MAX_LEN, mask_zero=True)(input2)\n",
    "\n",
    "# concat_layer = keras.layers.concatenate([word_emb, posd_emb],axis=1)\n",
    "\n",
    "# model = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(concat_layer)\n",
    "\n",
    "word_input = Input(shape=(MAX_LEN,))\n",
    "word_emb = Embedding(input_dim=n_words + 1, output_dim=20, input_length=MAX_LEN, mask_zero=True)(word_input)\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(word_emb)\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "model = Model(inputs=[word_input], outputs=[out])\n",
    "\n",
    "pos_input = Input(shape=(MAX_LEN,))\n",
    "pos_emb = Embedding(input_dim=n_words + 1, output_dim=20, input_length=MAX_LEN, mask_zero=True)(pos_input)\n",
    "model2 = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(pos_emb)\n",
    "out2 = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model2)  # softmax output layer\n",
    "model2 = Model(inputs=[pos_input], outputs=[out2])\n",
    "\n",
    "from keras.layers import Add\n",
    "mergedOut = Add()([model.output,model2.output])\n",
    "newModel = Model([word_input,pos_input], mergedOut)\n",
    "\n",
    "\n",
    "# pos_input = Input(shape=(MAX_LEN,))\n",
    "# pos_emb = Embedding(input_dim=n_pos, output_dim=10, input_length=MAX_LEN, mask_zero=True)(pos_input)\n",
    "\n",
    "# model = keras.layers.concatenate([word_emb, pos_emb])\n",
    "\n",
    "# # model = keras.layers.Concatenate([word, pos])\n",
    "\n",
    "# # model1 = Embedding(input_dim=n_words, output_dim=50, input_length=MAX_LEN*2)(concate_layers)\n",
    "# # model1 = Dropout(0.1)(concate_layers)\n",
    "# # model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "# # model = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "# # model2 = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(model1)\n",
    "# # model = Model(input, out)\n",
    "# model = Model(inputs=[main_input,input2], outputs=out)\n",
    "# # model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\",tf.keras.metrics.Recall()])\n",
    "\n",
    "\n",
    "newModel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# # model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.Recall()])\n",
    "# print(\"compiled\")\n",
    "# print(len(X_tr[0]))\n",
    "\n",
    "X_tr_wrd = []\n",
    "X_tr_pos = []\n",
    "for array in X_tr:\n",
    "#     print(len(array))\n",
    "#     print(len(array[:MAX_LEN]))\n",
    "#     print(len(array[MAX_LEN:]))\n",
    "    X_tr_wrd.append(array[:MAX_LEN])\n",
    "    X_tr_pos.append(array[MAX_LEN:-MAX_LEN])\n",
    "print(X_tr[0])\n",
    "# print(y_tr[0])\n",
    "newModel.summary()\n",
    "y_tr_tmp=[]\n",
    "y_tr_tmp.append(y_tr)\n",
    "y_tr_tmp.append(y_tr)\n",
    "# y_tr_tmp.append(y_tr)\n",
    "# print(np.array(y_tr_tmp)[0])\n",
    "\n",
    "history = newModel.fit([X_tr_wrd,X_tr_pos], np.array(y_tr), batch_size=32, epochs=3, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
    "# from keras.models import Model, Input\n",
    "# from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "# input = Input(shape=(inputDim,))\n",
    "# model = Embedding(input_dim=n_words, output_dim=50, input_length=inputDim)(input)\n",
    "# model = Dropout(0.1)(model)\n",
    "# model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "# out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "# model = Model(input, out)\n",
    "# model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\",tf.keras.metrics.Recall()])\n",
    "# print(\"compiled\")\n",
    "# print(len(X_tr[0]))\n",
    "# history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=1, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Working\n",
    "# hist = pd.DataFrame(history.history)\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.plot(hist[\"accuracy\"])\n",
    "# plt.plot(hist[\"val_accuracy\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn_crfsuite import CRF\n",
    "\n",
    "# crf = CRF(algorithm='lbfgs',\n",
    "#           c1=0.1,\n",
    "#           c2=0.1,\n",
    "#           max_iterations=100,\n",
    "#           all_possible_transitions=False)\n",
    "# pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "# report = flat_classification_report(y_pred=pred, y_true=y)\n",
    "# print(report)\n",
    "# crf.fit(X, y)\n",
    "pred = newModel.predict([X_tr_wrd,X_tr_pos])\n",
    "#print(pred)\n",
    "print(len(pred))\n",
    "# print(pred)\n",
    "pred = [[ np.argmax(row) for row in tmp] for tmp in pred ]\n",
    "# print(pred)\n",
    "# pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)\n",
    "# print(pred[0])\n",
    "# print(y_te[0][0])\n",
    "# pred2 = [to_categorical(i, num_classes=n_tags) for i in pred]\n",
    "# print(pred2[0][0])\n",
    "\n",
    "# print(len(y_te))\n",
    "# print(len(pred))\n",
    "# print(len(y_te[22]))\n",
    "# print(len(pred[22]))\n",
    "\n",
    "\n",
    "y_tr_real = []\n",
    "count_y = 0\n",
    "count_p = 0\n",
    "for sent_array in y_tr:\n",
    "    for array in sent_array:\n",
    "        nerTag = np.argmax(array)\n",
    "        found = 0\n",
    "        for tag in tag2idx:\n",
    "            if nerTag == tag2idx[tag]:\n",
    "                count_y = count_y+1\n",
    "                found=1\n",
    "                y_tr_real.append(tag)\n",
    "                break\n",
    "        if found == 0:\n",
    "            raise Exception(\"found is 0\")\n",
    "# print(y_te_real)\n",
    "pred_real = []\n",
    "for sent_array in pred:\n",
    "    for guess in sent_array:\n",
    "        nerTag = guess\n",
    "#         print(guess)\n",
    "        found = 0\n",
    "        for tag in tag2idx:\n",
    "#         print(nerTag)\n",
    "            if nerTag == tag2idx[tag]:\n",
    "                count_p = count_p+1\n",
    "                found=1\n",
    "                pred_real.append(tag)\n",
    "                break\n",
    "        if found == 0:\n",
    "            raise Exception(\"found is 0\")\n",
    "#     y_te_real.append()\n",
    "# print(count_y)\n",
    "# print(count_p)\n",
    "# print(len((pred_real)))\n",
    "#print(len(flatten(y_te_real)))\n",
    "# y_te_real = np.matrix(y_te_real)\n",
    "# pred_real = np.matrix(pred_real)\n",
    "# print(pred_real)\n",
    "\n",
    "report = classification_report(y_pred=pred_real, y_true=y_tr_real)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 2\n",
    "# p = model.predict(np.array([X_te[i]]))\n",
    "# p = np.argmax(p, axis=-1)\n",
    "print(\"{:15} ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "# for w, pred in zip(X_te[i], p[0]):\n",
    "#     print(\"{:15}: {}\".format(words[w], tags[pred]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model.evaluate(xTest,yTest, verbose=0)\n",
    "# results = model.evaluate(X_te,y_te, verbose=1)\n",
    "# print('test loss, test acc, test recall:', results)\n",
    "# X_tr, X_te, y_tr, y_te\n",
    "#Only Acc, 142, 7000, feat:1,2,-1 : [0.22299354140212138, 0.9714275]\n",
    "#[0.2284703826159239, 0.9709106, 0.92750543]\n",
    "#Recall [0.22716047498087089, 0.9273468]\n",
    "\n",
    "# 142, standardized [0.16994940752536059, 0.9674706, 0.9300009]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/floydhub/named-entity-recognition-template/blob/master/ner.ipynb\n",
    "# Saving Vocab\n",
    "#with open('models/word_to_index.pickle', 'wb') as handle:\n",
    "#    pickle.dump(word2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    " \n",
    "# Saving Vocab\n",
    "#with open('models/tag_to_index.pickle', 'wb') as handle:\n",
    "#    pickle.dump(tag2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Saving Model Weight\n",
    "#model.save_weights('models/lstm_crf_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
