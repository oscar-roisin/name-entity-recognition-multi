{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER - Multilang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import keras\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Mainly Used:\n",
    "# https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def sweStandardizeTag(line):\n",
    "    SWE_TAG_SET=[\"ORG\",\"PRS\",\"LOC\",\"OTHER\",\"O\"]\n",
    "    lineParts = line.split(\"\\t\")\n",
    "    ner_tag = lineParts[2]\n",
    "    if ner_tag in SWE_TAG_SET:\n",
    "        return line\n",
    "    if ner_tag[:3] == \"LOC\" or ner_tag == \"place\":\n",
    "        ner_tag = \"LOC\"\n",
    "    if ner_tag == \"person\":\n",
    "        ner_tag = \"PRS\"\n",
    "    if ner_tag in [\"inst\", \"ORG/PRS\", \"OBJ/ORG\"]:\n",
    "        ner_tag = \"ORG\"\n",
    "    if ner_tag in [\"WRK\",\"OBJ\",\"EVN\",\"product\",\"other\",\"work\",\"event\",\"myth\",\"animal\",\"MSR\",\"TME\",\"PRS/WRK\"]:\n",
    "        ner_tag = \"OTHER\"\n",
    "    \n",
    "    if ner_tag not in SWE_TAG_SET:\n",
    "        raise Exception(ner_tag + \" - not in tag set\") \n",
    "    return lineParts[0] + \"\\t\" + lineParts[1] + \"\\t\" + ner_tag\n",
    "\n",
    "def engStandardizeTag(line):\n",
    "    ENG_TAG_SET=[\"ORG\",\"PRS\",\"LOC\",\"OTHER\",\"O\"]\n",
    "    lineParts = line.split(\"\\t\")\n",
    "    ner_tag = lineParts[2]\n",
    "    ner_tag = ner_tag.strip()\n",
    "    if ner_tag[-3:] == \"geo\":\n",
    "        ner_tag = \"LOC\"\n",
    "    if ner_tag[-3:] == \"per\":\n",
    "        ner_tag = \"PRS\"\n",
    "    if ner_tag in [\"B-org\", \"I-org\"]:\n",
    "        ner_tag = \"ORG\"\n",
    "    if ner_tag in [\"B-gpe\",\"CD\",\"B-tim\",\"B-art\",\"I-art\",\"I-gpe\",\"I-tim\",\"B-nat\",\"I-nat\",\n",
    "                  \"B-eve\",\"I-eve\"]:\n",
    "        ner_tag = \"OTHER\"\n",
    "    if ner_tag in [\"000\\\"\",\"00\",\"\\\"\",\"JJ\",\"NNS\",\"CC\"] or ner_tag[:2].isdigit():\n",
    "        return \"\"\n",
    "    \n",
    "    if ner_tag not in ENG_TAG_SET:\n",
    "        raise Exception(ner_tag + \" - not in tag set\") \n",
    "    return lineParts[0].strip() + \"\\t\" + lineParts[1].strip() + \"\\t\" + ner_tag\n",
    "\n",
    "def frStandardizeTag(line):\n",
    "    SWE_TAG_SET=[\"ORG\",\"PRS\",\"LOC\",\"OTHER\",\"O\"]\n",
    "    lineParts = line.split(\"\\t\")\n",
    "    ner_tag = lineParts[2]\n",
    "    if ner_tag in SWE_TAG_SET:\n",
    "        return line\n",
    "    if ner_tag[-3:] == \"LOC\" or ner_tag == \"place\":\n",
    "        ner_tag = \"LOC\"\n",
    "    if ner_tag[-3:] == \"PER\" or ner_tag == \"person\":\n",
    "        ner_tag = \"PRS\"\n",
    "    if ner_tag[-3:] == \"ORG\":\n",
    "        ner_tag = \"ORG\"\n",
    "    if ner_tag in [\"I-MISC\",\"B-MISC\"]:\n",
    "        ner_tag = \"OTHER\"\n",
    "    \n",
    "    if ner_tag not in SWE_TAG_SET:\n",
    "        raise Exception(ner_tag + \" - not in tag set\") \n",
    "    return lineParts[0] + \"\\t\" + lineParts[1] + \"\\t\" + ner_tag\n",
    "\n",
    "\n",
    "DATA_DIR=\"../DATA\"\n",
    "filenames = [ \"swedish.txt\", \"english.txt\", \"french.txt\"]\n",
    "# filenames = [ \"french.txt\"]\n",
    "\n",
    "for langID in range(len(filenames)):\n",
    "    filename=filenames[langID]\n",
    "    originalFile= open(DATA_DIR + \"/\" + filename,\"r\")\n",
    "    file= open(DATA_DIR + \"/sent_\" + filename,\"w+\")\n",
    "    originalFileLines =originalFile.readlines()\n",
    "    sentId=1\n",
    "    for line in originalFileLines:\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            sentId = sentId + 1\n",
    "            continue\n",
    "        if langID == 0:\n",
    "            line = sweStandardizeTag(line)\n",
    "        if langID ==1:\n",
    "            line = engStandardizeTag(line)\n",
    "        if langID ==2:\n",
    "            line = frStandardizeTag(line)\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            sentId = sentId + 1\n",
    "            continue\n",
    "        file.write(str(sentId) + \"\\t\" + line +\"\\n\")\n",
    "# for i in range(10):\n",
    "#      file.write(\"This is line %d\\r\\n\" % (i+1))\n",
    "    file.close()\n",
    "    originalFile.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and Analyse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>ner</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Il</td>\n",
       "      <td>PRO:PER</td>\n",
       "      <td>O</td>\n",
       "      <td>Xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>assure</td>\n",
       "      <td>VER:pres</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>à</td>\n",
       "      <td>VER:pper</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>la</td>\n",
       "      <td>DET:ART</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>suite</td>\n",
       "      <td>NOM</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent   token       pos ner token_shape\n",
       "0     1      Il   PRO:PER   O          Xx\n",
       "1     1  assure  VER:pres   O           x\n",
       "2     1       à  VER:pper   O           x\n",
       "3     1      la   DET:ART   O           x\n",
       "4     1   suite       NOM   O           x"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR=\"../DATA\"\n",
    "SENT_COL=\"sent\"\n",
    "TOKEN_COL=\"token\"\n",
    "SHAPE_COL=\"token_shape\"\n",
    "POS_COL=\"pos\"\n",
    "NER_COL=\"ner\"\n",
    "\n",
    "TOKEN_SHAPE_CAT_COL=\"token_shape_cat\"\n",
    "POS_CAT_COL=\"pos_cat\"\n",
    "POS_PREV_CAT_COL=\"pos_prev_cat\"\n",
    "POS_NEXT_CAT_COL=\"pos_next_cat\"\n",
    "POS_PREV_PREV_CAT_COL=\"pos_prev_prev_cat\"\n",
    "NER_CAT_COL=\"ner_cat\"\n",
    "\n",
    "filenames = [ \"sent_swedish.txt\", \"sent_english.txt\", \"sent_french.txt\"]\n",
    "\n",
    "swe_data = pd.read_csv( DATA_DIR + '/' + filenames[0], sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "eng_data = pd.read_csv( DATA_DIR + '/' + filenames[1], sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "fr_data = pd.read_csv( DATA_DIR + '/' + filenames[2], sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "def analyseTokens(df):\n",
    "    #tokens = [\"\\\"\",\"'\",\",\",\".\",\"-\",\"_\"]\n",
    "    new_token_col = []\n",
    "    for token in df[TOKEN_COL]:\n",
    "        #print(token)\n",
    "        token=token.strip()\n",
    "        token_shape = \"\"\n",
    "        if len(token) == 1 and not token.isalnum():\n",
    "            token_shape = \"sign\" # or =token\n",
    "        else:\n",
    "            if token.isupper():\n",
    "                token_shape=\"X\"\n",
    "            elif token[0].isupper():\n",
    "                token_shape=\"Xx\"\n",
    "            else:\n",
    "                token_shape=\"x\"\n",
    "            if token[len(token)-1] == \".\":\n",
    "                token_shape+=\".\"\n",
    "        new_token_col.append(token_shape)\n",
    "    #print(len(new_token_col))\n",
    "    df[SHAPE_COL]=new_token_col\n",
    "\n",
    "def offsetArray(array,offset):\n",
    "    return np.roll(array,offset)\n",
    "    \n",
    "dataframes = [ swe_data, eng_data, fr_data ]\n",
    "# dataframes = [ swe_data ] #TODO remove\n",
    "\n",
    "for df in dataframes:\n",
    "    df.columns = [ SENT_COL, TOKEN_COL, POS_COL, NER_COL ]\n",
    "    analyseTokens(df)\n",
    "\n",
    "DATA_TESTED = fr_data\n",
    "\n",
    "DATA_TESTED.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [sent, token, pos, ner, token_shape]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "## TEMPLATE FOR TESTING ENG/FR\n",
    "\n",
    "# SWE-TAGSET:[\"ORG\",\"PRS\",\"LOC\",\"OTHER\",\"O\"] \n",
    "# TODO for END / FR\n",
    "sub_df = eng_data[eng_data[NER_COL] == \"OTHER\"]\n",
    "print(sub_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t,s) for w, p, t,s in zip(s[TOKEN_COL].values.tolist(),\n",
    "                                                           s[POS_COL].values.tolist(),\n",
    "                                                           s[NER_COL].values.tolist(),\n",
    "                                                           s[SHAPE_COL].values.tolist())]\n",
    "        self.grouped = self.data.groupby(SENT_COL).apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    shapetag = sent[i][3]\n",
    "    \n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'shapetag': shapetag,\n",
    "        'shapetag[:2]': shapetag,\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        shapetag1 = sent[i-1][3]\n",
    "        \n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:shapetag': shapetag1,\n",
    "            '-1:shapetag[:2]': shapetag1[:2],\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        shapetag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "             '+1:shapetag': shapetag1,\n",
    "            '+1:shapetag[:2]': shapetag1[:2],\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label, shape in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "getter = SentenceGetter(DATA_TESTED)\n",
    "sentences = getter.sentences\n",
    "\n",
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of sentencces: 134092\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len:  242\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(DATA_TESTED)\n",
    "# sent = getter.get_next()\n",
    "# print(sent)\n",
    "sentences = getter.sentences\n",
    "print(\"no. of sentencces: \" +str(len(sentences)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.show()\n",
    "\n",
    "print(\"max len: \",max([len(s) for s in sentences]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, Train length:73020 Test length: 8114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.1)\n",
    "print(\"Done, Train length:\" + str(len(xTrain)) + \" Test length: \" + str(len(xTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "crf = CRF(algorithm='lbfgs',\n",
    "          c1=10,\n",
    "          c2=0.1,\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=False)\n",
    "\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "# pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)\n",
    "# report = flat_classification_report(y_pred=pred, y_true=y)\n",
    "# print(report)\n",
    "print(\"Training\")\n",
    "crf.fit(xTrain, yTrain)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:['LOC', 'PRS', 'OTHER', 'ORG']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC      0.838     0.894     0.865      4527\n",
      "         ORG      0.781     0.750     0.765      3711\n",
      "         PRS      0.836     0.861     0.848      3535\n",
      "       OTHER      0.942     0.836     0.885      4663\n",
      "\n",
      "   micro avg      0.851     0.838     0.844     16436\n",
      "   macro avg      0.849     0.835     0.841     16436\n",
      "weighted avg      0.854     0.838     0.845     16436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "print(\"Labels:\" + str(labels))\n",
    "\n",
    "y_pred = crf.predict(xTest)\n",
    "# metrics.flat_f1_score(y, y_pred,\n",
    "#                       average='weighted', labels=labels)\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    yTest, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Ken tycker inte om Sverige\"\n",
    "\n",
    "file= open(\"tmpfile_swe.txt\",\"w+\")\n",
    "file.write(sent)\n",
    "file.close\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Tagging\n",
      "Loading Stagger model ...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# sent = input(\"Enter swedish sentence: \")\n",
    "# print(sent)\n",
    "\n",
    "print(\"Start Tagging\")\n",
    "# !../TAGGERS/stagger/tagSweNoFile.sh ./tmp/file_swe.txt fill\n",
    "!java -jar ../TAGGERS/stagger/stagger.jar -modelfile ../TAGGERS/stagger/models/swedish.bin -tag ./tmpfile_swe.txt > ./tmpfile_res_swe.txt\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PRS', 'O', 'O', 'O', 'LOC']]\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>ner</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ken</td>\n",
       "      <td>PM</td>\n",
       "      <td>PRS</td>\n",
       "      <td>Xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tycker</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>inte</td>\n",
       "      <td>AB</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>om</td>\n",
       "      <td>PL</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Sverige</td>\n",
       "      <td>PM</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Xx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent    token pos  ner token_shape\n",
       "0     1      Ken  PM  PRS          Xx\n",
       "1     1   tycker  VB    O           x\n",
       "2     1     inte  AB    O           x\n",
       "3     1       om  PL    O           x\n",
       "4     1  Sverige  PM  LOC          Xx"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file= open(\"tmpfile_res_swe.txt\",\"r\")\n",
    "result = file.readlines()\n",
    "\n",
    "data = [] \n",
    "  \n",
    "# Create the pandas DataFrame \n",
    "\n",
    "\n",
    "for line in result:\n",
    "    splitLine = line.split()\n",
    "    if(len(splitLine)==0):\n",
    "        break\n",
    "    word = splitLine[1]\n",
    "    pos = splitLine[3]\n",
    "    ner = \"unknown\"\n",
    "    data.append([1,word,pos,ner])\n",
    "file.close\n",
    "\n",
    "\n",
    "# newDf = pd.DataFrame()\n",
    "df = pd.DataFrame(data, columns = [ SENT_COL, TOKEN_COL, POS_COL, NER_COL ]) \n",
    "\n",
    "analyseTokens(df)\n",
    "\n",
    "\n",
    "\n",
    "getter_sent = SentenceGetter(df)\n",
    "pred_sentence = getter_sent.sentences\n",
    "\n",
    "X_sent = [sent2features(s) for s in pred_sentence]\n",
    "# y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "y_pred = crf.predict(X_sent)\n",
    "print(y_pred)\n",
    "print(\"done\")\n",
    "df[NER_COL]=y_pred[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved crf_fr.pickle\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/floydhub/named-entity-recognition-template/blob/master/ner.ipynb\n",
    "\n",
    "# filename=\"crf_swe.pickle\"\n",
    "# filename=\"crf_eng.pickle\"\n",
    "# filename=\"crf_fr.pickle\"\n",
    "\n",
    "import pickle\n",
    "with open(filename, 'wb') as handle:\n",
    "   pickle.dump(crf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Saved \" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded crf_fr.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# filename=\"crf_swe.pickle\"\n",
    "# filename=\"crf_eng.pickle\"\n",
    "filename=\"crf_fr.pickle\"\n",
    "\n",
    "file = open(filename, 'rb')\n",
    "crf = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "print(\"Loaded \" + filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
