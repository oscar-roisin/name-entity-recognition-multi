{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER - Multilang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import keras\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Mainly Used:\n",
    "# https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# SWE-TAGSET:[\"ORG\",\"PRS\",\"LOC\",\"OTHER\",\"O\"] \n",
    "def sweStandardizeTag(line):\n",
    "    SWE_TAG_SET=[\"ORG\",\"PRS\",\"LOC\",\"OTHER\",\"O\"]\n",
    "    lineParts = line.split(\"\\t\")\n",
    "    ner_tag = lineParts[2]\n",
    "    if ner_tag in SWE_TAG_SET:\n",
    "        return line\n",
    "    if ner_tag[:3] == \"LOC\" or ner_tag == \"place\":\n",
    "        ner_tag = \"LOC\"\n",
    "    if ner_tag == \"person\":\n",
    "        ner_tag = \"PRS\"\n",
    "    if ner_tag in [\"inst\", \"ORG/PRS\", \"OBJ/ORG\"]:\n",
    "        ner_tag = \"ORG\"\n",
    "    if ner_tag in [\"WRK\",\"OBJ\",\"EVN\",\"product\",\"other\",\"work\",\"event\",\"myth\",\"animal\",\"MSR\",\"TME\",\"PRS/WRK\"]:\n",
    "        ner_tag = \"OTHER\"\n",
    "    \n",
    "    if ner_tag not in SWE_TAG_SET:\n",
    "        raise Exception(ner_tag + \" - not in tag set\") \n",
    "    return lineParts[0] + \"\\t\" + lineParts[1] + \"\\t\" + ner_tag\n",
    "\n",
    "\n",
    "DATA_DIR=\"../DATA\"\n",
    "filenames = [ \"swedish.txt\"]#, \"french.txt\", \"english.txt\"]\n",
    "for filename in filenames:\n",
    "    originalFile= open(DATA_DIR + \"/\" + filename,\"r\")\n",
    "    file= open(DATA_DIR + \"/sent_\" + filename,\"w+\")\n",
    "    originalFileLines =originalFile.readlines()\n",
    "    sentId=1\n",
    "    for line in originalFileLines:\n",
    "        line = line.strip() # problem with this?\n",
    "        if len(line) == 0:\n",
    "            sentId = sentId + 1\n",
    "            continue\n",
    "        line = sweStandardizeTag(line)\n",
    "        file.write(str(sentId) + \"\\t\" + line +\"\\n\")\n",
    "# for i in range(10):\n",
    "#      file.write(\"This is line %d\\r\\n\" % (i+1))\n",
    "    file.close()\n",
    "    originalFile.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>ner</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>PP</td>\n",
       "      <td>O</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sin</td>\n",
       "      <td>PS</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>första</td>\n",
       "      <td>RO</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>reaktion</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>på</td>\n",
       "      <td>PP</td>\n",
       "      <td>O</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent     token pos ner token_shape\n",
       "0     1         I  PP   O           X\n",
       "1     1       sin  PS   O           x\n",
       "2     1    första  RO   O           x\n",
       "3     1  reaktion  NN   O           x\n",
       "4     1        på  PP   O           x"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR=\"../DATA\"\n",
    "SENT_COL=\"sent\"\n",
    "TOKEN_COL=\"token\"\n",
    "SHAPE_COL=\"token_shape\"\n",
    "POS_COL=\"pos\"\n",
    "NER_COL=\"ner\"\n",
    "\n",
    "TOKEN_SHAPE_CAT_COL=\"token_shape_cat\"\n",
    "POS_CAT_COL=\"pos_cat\"\n",
    "POS_PREV_CAT_COL=\"pos_prev_cat\"\n",
    "POS_NEXT_CAT_COL=\"pos_next_cat\"\n",
    "POS_PREV_PREV_CAT_COL=\"pos_prev_prev_cat\"\n",
    "NER_CAT_COL=\"ner_cat\"\n",
    "\n",
    "filenames = [ \"sent_swedish.txt\", \"sent_english.txt\", \"sent_french.txt\"]\n",
    "\n",
    "swe_data = pd.read_csv( DATA_DIR + '/' + filenames[0], sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "eng_data = pd.read_csv( DATA_DIR + '/' + filenames[1], sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "fr_data = pd.read_csv( DATA_DIR + '/' + filenames[2], sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "def analyseTokens(df):\n",
    "    #tokens = [\"\\\"\",\"'\",\",\",\".\",\"-\",\"_\"]\n",
    "    new_token_col = []\n",
    "    for token in df[TOKEN_COL]:\n",
    "        #print(token)\n",
    "        token=token.strip()\n",
    "        token_shape = \"\"\n",
    "        if len(token) == 1 and not token.isalnum():\n",
    "            token_shape = \"sign\" # or =token\n",
    "        else:\n",
    "            if token.isupper():\n",
    "                token_shape=\"X\"\n",
    "            elif token[0].isupper():\n",
    "                token_shape=\"Xx\"\n",
    "            else:\n",
    "                token_shape=\"x\"\n",
    "            if token[len(token)-1] == \".\":\n",
    "                token_shape+=\".\"\n",
    "        new_token_col.append(token_shape)\n",
    "    #print(len(new_token_col))\n",
    "    df[SHAPE_COL]=new_token_col\n",
    "\n",
    "def offsetArray(array,offset):\n",
    "    return np.roll(array,offset)\n",
    "    \n",
    "dataframes = [ swe_data, eng_data, fr_data ]\n",
    "dataframes = [ swe_data ] #TODO remove\n",
    "\n",
    "for df in dataframes:\n",
    "    df.columns = [ SENT_COL, TOKEN_COL, POS_COL, NER_COL ]\n",
    "    analyseTokens(df)\n",
    "\n",
    "# words = list(set(swe_data[\"token\"].values))\n",
    "# n_words = len(words);\n",
    "# print(n_words)\n",
    "\n",
    "DATA_TESTED = swe_data.head(100000)\n",
    "words = DATA_TESTED[TOKEN_COL].values.tolist()\n",
    "tags = DATA_TESTED[NER_COL].values.tolist()\n",
    "\n",
    "swe_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 1\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            sub_df = self.df[self.df[SENT_COL] == self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return sub_df[TOKEN_COL].values.tolist(), sub_df[POS_COL].values.tolist(), sub_df[NER_COL].values.tolist()    \n",
    "        except:\n",
    "            self.empty = True\n",
    "            print(\"EMPTY\")\n",
    "            return None, None, None\n",
    "getter = SentenceGetter(swe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEMPLATE FOR TESTING ENG/FR\n",
    "\n",
    "# SWE-TAGSET:[\"ORG\",\"PRS\",\"LOC\",\"OTHER\",\"O\"] \n",
    "# TODO for END / FR\n",
    "# sub_df = swe_data[swe_data[NER_COL] == \"OTHER\"]\n",
    "# print(sub_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_map(word):\n",
    "    '''Simple feature map.'''\n",
    "    return np.array([word.istitle(), word.islower(), word.isupper(), len(word),\n",
    "                     word.isdigit(),  word.isalpha()])\n",
    "\n",
    "## UNCOMMENT FOR SIMPLE CLASSIFICATION\n",
    "# words = [feature_map(w) for w in DATA_TESTED[TOKEN_COL].values.tolist()]\n",
    "# pred = cross_val_predict(RandomForestClassifier(n_estimators=20),\n",
    "#                          X=words, y=tags, cv=5)\n",
    "# report = classification_report(y_pred=pred, y_true=tags)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t,s) for w, p, t,s in zip(s[TOKEN_COL].values.tolist(),\n",
    "                                                           s[POS_COL].values.tolist(),\n",
    "                                                           s[NER_COL].values.tolist(),\n",
    "                                                           s[SHAPE_COL].values.tolist())]\n",
    "        self.grouped = self.data.groupby(SENT_COL).apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "# def word2features(sent, i):\n",
    "#     word = sent[i][0]\n",
    "#     postag = sent[i][1]\n",
    "\n",
    "#     features = {\n",
    "#         'bias': 1.0,\n",
    "#         'word.lower()': word.lower(),\n",
    "#         'word[-3:]': word[-3:],\n",
    "#         'word[-2:]': word[-2:],\n",
    "#         'word.isupper()': word.isupper(),\n",
    "#         'word.istitle()': word.istitle(),\n",
    "#         'word.isdigit()': word.isdigit(),\n",
    "#         'postag': postag,\n",
    "#         'postag[:2]': postag[:2],\n",
    "#     }\n",
    "#     if i > 0:\n",
    "#         word1 = sent[i-1][0]\n",
    "#         postag1 = sent[i-1][1]\n",
    "#         features.update({\n",
    "#             '-1:word.lower()': word1.lower(),\n",
    "#             '-1:word.istitle()': word1.istitle(),\n",
    "#             '-1:word.isupper()': word1.isupper(),\n",
    "#             '-1:postag': postag1,\n",
    "#             '-1:postag[:2]': postag1[:2],\n",
    "#         })\n",
    "#     else:\n",
    "#         features['BOS'] = True\n",
    "\n",
    "#     if i < len(sent)-1:\n",
    "#         word1 = sent[i+1][0]\n",
    "#         postag1 = sent[i+1][1]\n",
    "#         features.update({\n",
    "#             '+1:word.lower()': word1.lower(),\n",
    "#             '+1:word.istitle()': word1.istitle(),\n",
    "#             '+1:word.isupper()': word1.isupper(),\n",
    "#             '+1:postag': postag1,\n",
    "#             '+1:postag[:2]': postag1[:2],\n",
    "#         })\n",
    "#     else:\n",
    "#         features['EOS'] = True\n",
    "\n",
    "#     return features\n",
    "\n",
    "\n",
    "# def sent2features(sent):\n",
    "#     return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "# def sent2labels(sent):\n",
    "#     return [label for token, postag, label in sent]\n",
    "\n",
    "# def sent2tokens(sent):\n",
    "#     return [token for token, postag, label in sent]\n",
    "\n",
    "# X = [sent2features(s) for s in sentences]\n",
    "# y = [sent2labels(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data into Train/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "https://www.depends-on-the-definition.com/guide-sequence-tagging-neural-networks-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of sentencces: 6862\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len:  103\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(DATA_TESTED)\n",
    "sent = getter.get_next()\n",
    "# print(sent)\n",
    "sentences = getter.sentences\n",
    "print(\"no. of sentencces: \" +str(len(sentences)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.show()\n",
    "\n",
    "print(\"max len: \",max([len(s) for s in sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "{'LOC': 0, 'PRS': 1, 'ORG': 2, 'O': 3, 'OTHER': 4}\n",
      "{'PN': 0, 'DT': 1, 'MAD': 2, 'IE': 3, 'IN': 4, 'PC': 5, 'HP': 6, 'JJ': 7, 'RO': 8, 'HD': 9, 'VB': 10, 'NN': 11, 'MID': 12, 'HS': 13, 'AB': 14, 'RG': 15, 'PS': 16, 'HA': 17, 'PAD': 18, 'PP': 19, 'PL': 20, 'UO': 21, 'SN': 22, 'PM': 23, 'KN': 24, 'POSPAD': 25}\n",
      "{'X.': 0, 'x.': 1, 'sign': 2, 'X': 3, 'Xx': 4, 'x': 5, 'SHAPEPAD': 6}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "WORD_PAD = \"WORDPAD\"\n",
    "POS_PAD = \"POSPAD\"\n",
    "SHAPE_PAD = \"SHAPEPAD\"\n",
    "\n",
    "words = list(set(DATA_TESTED[TOKEN_COL].values))\n",
    "words.append(WORD_PAD)\n",
    "tags = list(set(DATA_TESTED[NER_COL].values))\n",
    "pos = list(set(DATA_TESTED[POS_COL].values))\n",
    "pos.append(POS_PAD)\n",
    "shape = list(set(DATA_TESTED[SHAPE_COL].values))\n",
    "shape.append(SHAPE_PAD)\n",
    "n_words = len(words)\n",
    "n_tags = len(tags)\n",
    "n_pos = len(pos)\n",
    "n_shape = len(shape)\n",
    "print(n_pos)\n",
    "# MAX_LEN = max([len(s) for s in sentences])\n",
    "MAX_LEN = 80\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "pos2idx = {t: i for i, t in enumerate(pos)}\n",
    "shape2idx = {t: i for i, t in enumerate(shape)}\n",
    "# print(word2idx[\"i\"])\n",
    "print(tag2idx)\n",
    "print(pos2idx)\n",
    "print(shape2idx)\n",
    "\n",
    "\n",
    "\n",
    "# n_words = len(words); n_words\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# X_word = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "# X_shape = [[shape2idx[w[3]] for w in s] for s in sentences]\n",
    "# X_pos = [[pos2idx[w[1]] for w in s] for s in sentences]\n",
    "# y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "\n",
    "X_sent_nr = DATA_TESTED[SENT_COL].values\n",
    "X_word = [word2idx[w] for w in DATA_TESTED[TOKEN_COL].values]\n",
    "X_shape = [shape2idx[w] for w in DATA_TESTED[SHAPE_COL].values]\n",
    "X_pos = [pos2idx[w] for w in DATA_TESTED[POS_COL].values]\n",
    "y = [tag2idx[w] for w in DATA_TESTED[NER_COL].values]\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "\n",
    "def limitSentenceLength( arrays, max_len):\n",
    "    newArrays = []\n",
    "    for sent_array in arrays:\n",
    "        if(len(sent_array)>max_len):\n",
    "            newArrays.append(sent_array[:max_len])\n",
    "        else:\n",
    "            newArrays.append(sent_array)\n",
    "    return newArrays\n",
    "\n",
    "#Limit sentence to MAX_LEN\n",
    "# y = limitSentenceLength(y, MAX_LEN)\n",
    "# X_word = limitSentenceLength(X_word, MAX_LEN)\n",
    "# X_pos = limitSentenceLength(X_pos, MAX_LEN)\n",
    "# X_shape = limitSentenceLength(X_shape, MAX_LEN)\n",
    "\n",
    "#Pad to MAX_LEN\n",
    "# y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "# X_word = pad_sequences(maxlen=MAX_LEN, sequences=X_word, padding=\"post\", value=n_words - 1)\n",
    "# X_pos = pad_sequences(maxlen=MAX_LEN, sequences=X_pos, padding=\"post\", value=n_pos - 1)\n",
    "# X_shape = pad_sequences(maxlen=MAX_LEN, sequences=X_pos, padding=\"post\", value=n_pos - 1)\n",
    "\n",
    "# y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "\n",
    "# X = np.column_stack((X_word,X_pos,X_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6904, 3, 19, 19371, 765, 25, 5, 6, 16, 19371, 17148, 25, 5, 6, 8]\n",
      "inputLen: 15\n",
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# X_sent_nr = DATA_TESTED[SENT_COL].values\n",
    "# X_word = [word2idx[w] for w in DATA_TESTED[TOKEN_COL].values]\n",
    "# X_shape = [shape2idx[w] for w in DATA_TESTED[SHAPE_COL].values]\n",
    "# X_pos = [pos2idx[w] for w in DATA_TESTED[POS_COL].values]\n",
    "# y = [tag2idx[w] for w in DATA_TESTED[NER_COL].values]\n",
    "HISTORY_LIMIT = 3 # looking N-1 in both directions\n",
    "\n",
    "def getSentLimit(sent_nr_array,i):\n",
    "    currentSentId = sent_nr_array[i]\n",
    "    limitPrev=0\n",
    "    limitNext=0\n",
    "    for j in range(HISTORY_LIMIT):\n",
    "        if j == 0 :\n",
    "            continue\n",
    "        if i-j>=0 and sent_nr_array[i-j] == currentSentId:\n",
    "            limitPrev = j\n",
    "        if i+j<len(sent_nr_array) and sent_nr_array[i+j] == currentSentId:\n",
    "            limitNext = j\n",
    "    \n",
    "    return limitPrev, limitNext\n",
    "\n",
    "\n",
    "newX = []\n",
    "\n",
    "# word2idx = {w: i for i, w in enumerate(words)}\n",
    "# tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "# pos2idx = {t: i for i, t in enumerate(pos)}\n",
    "# shape2idx = {t: i for i, t in enumerate(shape)}\n",
    "X_features = [X_word,X_shape,X_pos]\n",
    "featurePad = [word2idx[WORD_PAD],pos2idx[POS_PAD],shape2idx[SHAPE_PAD]]\n",
    "\n",
    "# y = []\n",
    "for i in range(len(X_sent_nr)):\n",
    "    limitPrev, limitNext = getSentLimit(X_sent_nr,i)\n",
    "    \n",
    "    X_feature_row =  []\n",
    "    \n",
    "    for featId in range(len(X_features)):\n",
    "        X_feature_row.append(X_features[featId][i])\n",
    "    \n",
    "    for j in range(HISTORY_LIMIT):\n",
    "        if j == 0 :\n",
    "            continue\n",
    "        for featId in range(len(X_features)):\n",
    "            featureList = X_features[featId]\n",
    "            if j<=limitPrev:\n",
    "                X_feature_row.append(featureList[i-j])\n",
    "            else:\n",
    "                X_feature_row.append(featurePad[featId])\n",
    "            if j<=limitNext:\n",
    "                X_feature_row.append(featureList[i+j])\n",
    "            else:\n",
    "                X_feature_row.append(featurePad[featId])\n",
    "    newX.append(X_feature_row)\n",
    "print(newX[0])\n",
    "\n",
    "inputLen = len(newX[0])\n",
    "print(\"inputLen: \"+ str(inputLen))\n",
    "\n",
    "\n",
    "print(y[0])\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(newX, y, test_size=0.1)\n",
    "print(y[0])\n",
    "print(np.array(y_tr)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from keras.models import Model, Input\n",
    "# from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "# from keras.models import Sequential\n",
    "\n",
    "# input1 = Input(shape=(MAX_LEN*3,))\n",
    "# # # word_emb = Embedding(input_dim=n_words + 1, output_dim=80, input_length=MAX_LEN, mask_zero=True)(input)\n",
    "\n",
    "# # # input2 = Input(shape=(MAX_LEN,))\n",
    "# # # pos_emb = Embedding(input_dim=n_pos, output_dim=80, input_length=MAX_LEN, mask_zero=True)(input2)\n",
    "# # # concate_layers = keras.layers.concatenate([word_emb, pos_emb],axis=1)\n",
    "model = Sequential()\n",
    "# # model = Sequential()\n",
    "# # # model.add(Bidirectional(LSTM(num_channels, \n",
    "# # #                                      implementation = 2, recurrent_activation = 'sigmoid'),\n",
    "# # #                                 input_shape=(input_length, input_dim)))\n",
    "\n",
    "model.add(Dense(inputLen*2, activation='relu', input_dim=inputLen))\n",
    "model.add(Dense(n_tags, activation='sigmoid'))\n",
    "\n",
    "# # main_input = Input(shape=(MAX_LEN,))\n",
    "# # word_emb = Embedding(input_dim=n_words + 1, output_dim=150,input_length=MAX_LEN, mask_zero=True)(main_input)\n",
    "\n",
    "# # input2 = Input(shape=(MAX_LEN,))\n",
    "# # posd_emb = Embedding(input_dim=n_pos, output_dim=150,input_length=MAX_LEN, mask_zero=True)(input2)\n",
    "\n",
    "# # concat_layer = keras.layers.concatenate([word_emb, posd_emb],axis=1)\n",
    "\n",
    "# # model = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(concat_layer)\n",
    "\n",
    "# word_input = Input(shape=(MAX_LEN,))\n",
    "# word_emb = Embedding(input_dim=n_words + 1, output_dim=20, input_length=MAX_LEN, mask_zero=True)(word_input)\n",
    "# model = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(word_emb)\n",
    "# out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "# model = Model(inputs=[word_input], outputs=[out])\n",
    "\n",
    "# pos_input = Input(shape=(MAX_LEN,))\n",
    "# pos_emb = Embedding(input_dim=n_words + 1, output_dim=20, input_length=MAX_LEN, mask_zero=True)(pos_input)\n",
    "# model2 = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(pos_emb)\n",
    "# out2 = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model2)  # softmax output layer\n",
    "# model2 = Model(inputs=[pos_input], outputs=[out2])\n",
    "\n",
    "# from keras.layers import Add\n",
    "# mergedOut = Add()([model.output,model2.output])\n",
    "# newModel = Model([word_input,pos_input], mergedOut)\n",
    "\n",
    "\n",
    "# # pos_input = Input(shape=(MAX_LEN,))\n",
    "# # pos_emb = Embedding(input_dim=n_pos, output_dim=10, input_length=MAX_LEN, mask_zero=True)(pos_input)\n",
    "\n",
    "# # model = keras.layers.concatenate([word_emb, pos_emb])\n",
    "\n",
    "# # # model = keras.layers.Concatenate([word, pos])\n",
    "\n",
    "# # # model1 = Embedding(input_dim=n_words, output_dim=50, input_length=MAX_LEN*2)(concate_layers)\n",
    "# # # model1 = Dropout(0.1)(concate_layers)\n",
    "# # # model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "# # # model = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "# # # model2 = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(model1)\n",
    "# # # model = Model(input, out)\n",
    "# # model = Model(inputs=[main_input,input2], outputs=out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\",tf.keras.metrics.Recall()])\n",
    "\n",
    "\n",
    "# newModel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# # # model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.Recall()])\n",
    "# # print(\"compiled\")\n",
    "# # print(len(X_tr[0]))\n",
    "\n",
    "# X_tr_wrd = []\n",
    "# X_tr_pos = []\n",
    "# for array in X_tr:\n",
    "# #     print(len(array))\n",
    "# #     print(len(array[:MAX_LEN]))\n",
    "# #     print(len(array[MAX_LEN:]))\n",
    "#     X_tr_wrd.append(array[:MAX_LEN])\n",
    "#     X_tr_pos.append(array[MAX_LEN:-MAX_LEN])\n",
    "# print(X_tr[0])\n",
    "# # print(y_tr[0])\n",
    "model.summary()\n",
    "# y_tr_tmp=[]\n",
    "# y_tr_tmp.append(y_tr)\n",
    "# y_tr_tmp.append(y_tr)\n",
    "# # y_tr_tmp.append(y_tr)\n",
    "# # print(np.array(y_tr_tmp)[0])\n",
    "\n",
    "history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=3, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
    "# from keras.models import Model, Input\n",
    "# from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "# input = Input(shape=(inputDim,))\n",
    "# model = Embedding(input_dim=n_words, output_dim=50, input_length=inputDim)(input)\n",
    "# model = Dropout(0.1)(model)\n",
    "# model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "# out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "# model = Model(input, out)\n",
    "# model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\",tf.keras.metrics.Recall()])\n",
    "# print(\"compiled\")\n",
    "# print(len(X_tr[0]))\n",
    "# history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=1, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Working\n",
    "# hist = pd.DataFrame(history.history)\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.plot(hist[\"accuracy\"])\n",
    "# plt.plot(hist[\"val_accuracy\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.00      0.00      0.00      2036\n",
      "           O       0.99      1.00      0.99    486984\n",
      "         ORG       0.00      0.00      0.00      1099\n",
      "       OTHER       0.00      0.00      0.00       476\n",
      "         PRS       0.00      0.00      0.00      3405\n",
      "\n",
      "    accuracy                           0.99    494000\n",
      "   macro avg       0.20      0.20      0.20    494000\n",
      "weighted avg       0.97      0.99      0.98    494000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn_crfsuite import CRF\n",
    "\n",
    "# crf = CRF(algorithm='lbfgs',\n",
    "#           c1=0.1,\n",
    "#           c2=0.1,\n",
    "#           max_iterations=100,\n",
    "#           all_possible_transitions=False)\n",
    "# pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "# report = flat_classification_report(y_pred=pred, y_true=y)\n",
    "# print(report)\n",
    "# crf.fit(X, y)\n",
    "pred = newModel.predict([X_tr_wrd,X_tr_pos])\n",
    "#print(pred)\n",
    "print(len(pred))\n",
    "# print(pred)\n",
    "pred = [[ np.argmax(row) for row in tmp] for tmp in pred ]\n",
    "# print(pred)\n",
    "# pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)\n",
    "# print(pred[0])\n",
    "# print(y_te[0][0])\n",
    "# pred2 = [to_categorical(i, num_classes=n_tags) for i in pred]\n",
    "# print(pred2[0][0])\n",
    "\n",
    "# print(len(y_te))\n",
    "# print(len(pred))\n",
    "# print(len(y_te[22]))\n",
    "# print(len(pred[22]))\n",
    "\n",
    "\n",
    "y_tr_real = []\n",
    "count_y = 0\n",
    "count_p = 0\n",
    "for sent_array in y_tr:\n",
    "    for array in sent_array:\n",
    "        nerTag = np.argmax(array)\n",
    "        found = 0\n",
    "        for tag in tag2idx:\n",
    "            if nerTag == tag2idx[tag]:\n",
    "                count_y = count_y+1\n",
    "                found=1\n",
    "                y_tr_real.append(tag)\n",
    "                break\n",
    "        if found == 0:\n",
    "            raise Exception(\"found is 0\")\n",
    "# print(y_te_real)\n",
    "pred_real = []\n",
    "for sent_array in pred:\n",
    "    for guess in sent_array:\n",
    "        nerTag = guess\n",
    "#         print(guess)\n",
    "        found = 0\n",
    "        for tag in tag2idx:\n",
    "#         print(nerTag)\n",
    "            if nerTag == tag2idx[tag]:\n",
    "                count_p = count_p+1\n",
    "                found=1\n",
    "                pred_real.append(tag)\n",
    "                break\n",
    "        if found == 0:\n",
    "            raise Exception(\"found is 0\")\n",
    "#     y_te_real.append()\n",
    "# print(count_y)\n",
    "# print(count_p)\n",
    "# print(len((pred_real)))\n",
    "#print(len(flatten(y_te_real)))\n",
    "# y_te_real = np.matrix(y_te_real)\n",
    "# pred_real = np.matrix(pred_real)\n",
    "# print(pred_real)\n",
    "\n",
    "report = classification_report(y_pred=pred_real, y_true=y_tr_real)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 2\n",
    "# p = model.predict(np.array([X_te[i]]))\n",
    "# p = np.argmax(p, axis=-1)\n",
    "print(\"{:15} ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "# for w, pred in zip(X_te[i], p[0]):\n",
    "#     print(\"{:15}: {}\".format(words[w], tags[pred]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model.evaluate(xTest,yTest, verbose=0)\n",
    "# results = model.evaluate(X_te,y_te, verbose=1)\n",
    "# print('test loss, test acc, test recall:', results)\n",
    "# X_tr, X_te, y_tr, y_te\n",
    "#Only Acc, 142, 7000, feat:1,2,-1 : [0.22299354140212138, 0.9714275]\n",
    "#[0.2284703826159239, 0.9709106, 0.92750543]\n",
    "#Recall [0.22716047498087089, 0.9273468]\n",
    "\n",
    "# 142, standardized [0.16994940752536059, 0.9674706, 0.9300009]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/floydhub/named-entity-recognition-template/blob/master/ner.ipynb\n",
    "# Saving Vocab\n",
    "#with open('models/word_to_index.pickle', 'wb') as handle:\n",
    "#    pickle.dump(word2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    " \n",
    "# Saving Vocab\n",
    "#with open('models/tag_to_index.pickle', 'wb') as handle:\n",
    "#    pickle.dump(tag2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Saving Model Weight\n",
    "#model.save_weights('models/lstm_crf_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
